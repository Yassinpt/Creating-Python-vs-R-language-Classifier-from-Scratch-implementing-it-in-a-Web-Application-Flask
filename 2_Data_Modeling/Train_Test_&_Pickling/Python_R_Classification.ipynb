{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict         \n",
    "from nltk.corpus import stopwords             #  la list des stops words est déja définie dans la bibliothèque nltk.corpus\n",
    "from nltk.stem import WordNetLemmatizer       #  WordNetLemmatizer c est la classe qui permet de faire la lemmitization\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_string(post):\n",
    "    \"\"\"\"\n",
    "        Les Parameters:\n",
    "        ----------\n",
    "        post: c 'est le texte qu'on veut nettoyer\n",
    "\n",
    "        la fonction fait quoi ?\n",
    "        -----------------------\n",
    "        Preprocess the string argument - str_arg - such that :\n",
    "        1. # tout str qui commence par http:// ou https://   \\S : éliminez les urls comme \"\"https://chrome.google.com/webstore/detail/simple-websocket-client/pfdhoblngboilpfeibdedpjgfnlcodoo\"\"\n",
    "        2. multiple spaces are replaced by single space\n",
    "        3. str_arg is converted to lower case\n",
    "\n",
    "        Example:\n",
    "        --------\n",
    "        Input :  Menu is absolutely perfect,loved it!\n",
    "        Output:  menu is absolutely perfect loved\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        ---------\n",
    "        un post néttoyé \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    post = post.lower()  # rendre post en minuscules\n",
    "\n",
    "    urlTemplate = r\"http(s)*://\\S*\"  # tout str qui commence par http:// ou https://   \\S : tout les match qui ne contient pas l'espace\n",
    "    postNoUrl = re.sub(urlTemplate, '',\n",
    "                       post)         # éliminez les urls comme \"\"https://chrome.google.com/webstore/detail/simple-websocket-client/pfdhoblngboilpfeibdedpjgfnlcodoo\"\"\n",
    "\n",
    "    cleanedPost = re.sub(r'[^a-z]+', ' ', postNoUrl)  # tout caractère à l'exception des alphabets est remplacé par ''\n",
    "\n",
    "    cleanedPost = re.sub('(\\s+)', ' ', cleanedPost)  # les  spaces multiples sont remplacés par un seul space\n",
    "\n",
    "   \n",
    "    return cleanedPost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "\n",
    "    def __init__(self, unique_classes):\n",
    "\n",
    "        self.classes = unique_classes  # c'est le constructeur de la classe NaiveBayes , le parametre unique_classes dans notre problème =np.array([\"python\",\"r\"])\n",
    "\n",
    "    def addToBow(self, example, dict_index):\n",
    "\n",
    "        '''\n",
    "            Parameters:\n",
    "            1. example\n",
    "            2. dict_index - implies to which BoW category this example belongs to\n",
    "\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            It simply splits the example on the basis of space as a tokenizer and adds every tokenized word to\n",
    "            its corresponding dictionary/BoW\n",
    "\n",
    "            Returns:\n",
    "            ---------\n",
    "            Nothing\n",
    "\n",
    "       '''\n",
    "\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        if isinstance(example,np.ndarray): example=example[0]\n",
    "\n",
    "        for token_word in example.split():  # for every word in preprocessed example\n",
    "            stopWords = set(stopwords.words('english'))\n",
    "            token_word = lemmatizer.lemmatize(token_word)  # here we do lemmatization for token_word\n",
    "            if token_word not in stopWords:\n",
    "                self.bow_dicts[dict_index][token_word] += 1  # increment in its count\n",
    "\n",
    "    def train(self, dataset, labels):\n",
    "\n",
    "        '''\n",
    "            Parameters:\n",
    "            1. dataset - shape = (m X d)\n",
    "            2. labels - shape = (m,)\n",
    "\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            This is the training function which will train the Naive Bayes Model i.e compute a BoW for each\n",
    "            category/class.\n",
    "\n",
    "            Returns:\n",
    "            ---------\n",
    "            Nothing\n",
    "\n",
    "        '''\n",
    "\n",
    "        self.examples = dataset\n",
    "        self.labels = labels\n",
    "        self.bow_dicts = np.array([defaultdict(int) for index in range(self.classes.shape[0])])            #classes=np.array [\"pyttho\",\"r\"]                     #Dictionaries are a convenient way to store data for later retrieval by name (key). Keys must be unique, immutable objects, and are typically strings. The values in a dictionary can be anything. For many applications the values are simple types such as integers and strings.\n",
    "\n",
    "                                                                                                                            #It gets more interesting when the values in a dictionary are collections (lists, dicts, etc.) In this case, the value (an empty list or dict) must be initialized the first time a given key is used. While this is relatively easy to do manually, the defaultdict type automates and simplifies these kinds of operations.\n",
    "\n",
    "                                                                                                                            #A defaultdict works exactly like a normal dict, but it is initialized with a function (“default factory”) that takes no arguments and provides the default value for a nonexistent key.\n",
    "                                                                                                                         #A defaultdict will never raise a KeyError. Any key that does not exist gets the value returned by the default factory.\n",
    "\n",
    "        # only convert to numpy arrays if initially not passed as numpy arrays - else its a useless recomputation\n",
    "\n",
    "        if not isinstance(self.examples, np.ndarray): self.examples = np.array(self.examples)\n",
    "        if not isinstance(self.labels, np.ndarray): self.labels = np.array(self.labels)\n",
    "\n",
    "        # constructing BoW for each category\n",
    "        for cat_index, cat in enumerate(self.classes):\n",
    "            all_cat_examples = self.examples[self.labels == cat]  # filter all examples of category == cat\n",
    "\n",
    "            # get examples preprocessed\n",
    "\n",
    "            cleaned_examples = [preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "\n",
    "            cleaned_examples = pd.DataFrame(data=cleaned_examples)\n",
    "\n",
    "            # now costruct BoW of this particular category\n",
    "            np.apply_along_axis(self.addToBow, 1, cleaned_examples, cat_index)\n",
    "\n",
    "        ###################################################################################################\n",
    "\n",
    "        '''\n",
    "            Although we are done with the training of Naive Bayes Model BUT!!!!!!\n",
    "            ------------------------------------------------------------------------------------\n",
    "            Remember The Test Time Forumla ? : {for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ] } * p(c)\n",
    "            ------------------------------------------------------------------------------------\n",
    "\n",
    "            We are done with constructing of BoW for each category. But we need to precompute a few \n",
    "            other calculations at training time too:\n",
    "            1. prior probability of each class - p(c)\n",
    "            2. vocabulary |V| \n",
    "            3. denominator value of each class - [ count(c) + |V| + 1 ] \n",
    "\n",
    "            Reason for doing this precomputing calculations stuff ???\n",
    "            ---------------------\n",
    "            We can do all these 3 calculations at test time too BUT doing so means to re-compute these \n",
    "            again and again every time the test function will be called - this would significantly\n",
    "            increase the computation time especially when we have a lot of test examples to classify!!!).  \n",
    "            And moreover, it doensot make sense to repeatedly compute the same thing - \n",
    "            why do extra computations ???\n",
    "            So we will precompute all of them & use them during test time to speed up predictions.\n",
    "\n",
    "        '''\n",
    "\n",
    "        ###################################################################################################\n",
    "\n",
    "        prob_classes = np.empty(self.classes.shape[0])\n",
    "        all_words = []\n",
    "        cat_word_counts = np.empty(self.classes.shape[0])\n",
    "        for cat_index, cat in enumerate(self.classes):\n",
    "            # Calculating prior probability p(c) for each class\n",
    "            prob_classes[cat_index] = np.sum(self.labels == cat) / float(self.labels.shape[0])\n",
    "\n",
    "            # Calculating total counts of all the words of each class\n",
    "            count = list(self.bow_dicts[cat_index].values())\n",
    "            cat_word_counts[cat_index] = np.sum(\n",
    "                np.array(list(self.bow_dicts[cat_index].values()))) + 1  # |v| is remaining to be added\n",
    "\n",
    "            # get all words of this category\n",
    "            all_words += self.bow_dicts[cat_index].keys()\n",
    "\n",
    "        # combine all words of every category & make them unique to get vocabulary -V- of entire training set\n",
    "\n",
    "        self.vocab = np.unique(np.array(all_words))\n",
    "        self.vocab_length = self.vocab.shape[0]\n",
    "\n",
    "        # computing denominator value\n",
    "        denoms = np.array(\n",
    "            [cat_word_counts[cat_index] + self.vocab_length + 1 for cat_index, cat in enumerate(self.classes)])\n",
    "\n",
    "        '''\n",
    "            Now that we have everything precomputed as well, its better to organize everything in a tuple \n",
    "            rather than to have a separate list for every thing.\n",
    "\n",
    "            Every element of self.cats_info has a tuple of values\n",
    "            Each tuple has a dict at index 0, prior probability at index 1, denominator value at index 2\n",
    "        '''\n",
    "\n",
    "        self.cats_info = [(self.bow_dicts[cat_index], prob_classes[cat_index], denoms[cat_index]) for cat_index, cat in\n",
    "                          enumerate(self.classes)]\n",
    "        self.cats_info = np.array(self.cats_info)\n",
    "\n",
    "    def getExampleProb(self, test_example):\n",
    "\n",
    "        '''\n",
    "            Parameters:\n",
    "            -----------\n",
    "            1. a single test example\n",
    "\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            Function that estimates posterior probability of the given test example\n",
    "\n",
    "            Returns:\n",
    "            ---------\n",
    "            probability of test example in ALL CLASSES\n",
    "        '''\n",
    "\n",
    "        likelihood_prob = np.zeros(self.classes.shape[0])  # to store probability w.r.t each class\n",
    "\n",
    "        # finding probability w.r.t each class of the given test example\n",
    "        for cat_index, cat in enumerate(self.classes):\n",
    "\n",
    "            for test_token in test_example.split():  # split the test example and get p of each test word\n",
    "\n",
    "                ####################################################################################\n",
    "\n",
    "                # This loop computes : for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ]\n",
    "\n",
    "                ####################################################################################\n",
    "\n",
    "                # get total count of this test token from it's respective training dict to get numerator value\n",
    "                test_token_counts = self.cats_info[cat_index][0].get(test_token, 0) + 1\n",
    "\n",
    "                # now get likelihood of this test_token word\n",
    "                test_token_prob = test_token_counts / float(self.cats_info[cat_index][2])\n",
    "\n",
    "                # remember why taking log? To prevent underflow!\n",
    "                likelihood_prob[cat_index] += np.log(test_token_prob)\n",
    "\n",
    "        # we have likelihood estimate of the given example against every class but we need posterior probility\n",
    "        post_prob = np.empty(self.classes.shape[0])\n",
    "        for cat_index, cat in enumerate(self.classes):\n",
    "            post_prob[cat_index] = likelihood_prob[cat_index] + np.log(self.cats_info[cat_index][1])\n",
    "        \n",
    "        return post_prob\n",
    "\n",
    "    def test(self, test_set):\n",
    "\n",
    "        '''\n",
    "            Parameters:\n",
    "            -----------\n",
    "            1. A complete test set of shape (m,)\n",
    "\n",
    "\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            Determines probability of each test example against all classes and predicts the label\n",
    "            against which the class probability is maximum\n",
    "\n",
    "            Returns:\n",
    "            ---------\n",
    "            Predictions of test examples - A single prediction against every test example\n",
    "        '''\n",
    "\n",
    "        predictions = []  # to store prediction of each test example\n",
    "        for example in test_set:\n",
    "            # preprocess the test example the same way we did for training set exampels\n",
    "          \n",
    "            cleaned_example = preprocess_string(example)\n",
    "\n",
    "            # simply get the posterior probability of every example\n",
    "            post_prob = self.getExampleProb(cleaned_example)  # get prob of this example for both classes\n",
    "            \n",
    "            # simply pick the max value and map against self.classes!\n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "\n",
    "        return np.array(predictions),post_prob\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "Unique Classes:  ['python' 'r']\n",
      "Total Number of  Examples:  (338213,)\n",
      "------------------Training In Progress------------------------\n",
      "Training Examples:  (253659,)\n",
      "------------------------Training Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yassine Z\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:80: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Examples:  84554\n",
      "Test Set Accuracy:  0.0\n",
      "------------------Pickling nb------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "#import Naive_Bayes as nbs\n",
    "\n",
    "\n",
    "\n",
    "# lecture de tous les fichiers post_python_x.csv et post_r_x.csv dans objet data de type DataFrame\n",
    "header=[\"post\",\"python or r\"]\n",
    "\n",
    "# Lecture des fichiers post_r_x.csv dans une liste rData\n",
    "path_r=r\"C:\\Users\\Yassine Z\\Desktop\\Data_Posts_StackOverFlow\\Data_R\"\n",
    "r_files_name=[\"Posts_R_Page_{}.csv\".format(i) for i in range(1,3501)]\n",
    "rData=[]\n",
    "for file_name in r_files_name :\n",
    "\n",
    "   with open(path_r+\"/\"+file_name, 'r',encoding='utf-8') as readFile:\n",
    "       reader = csv.reader(readFile)\n",
    "       lines = list(reader)\n",
    "       for line in lines[1:] :\n",
    "            if len(line)==2 :                   # car entre une ligne et une autre il y a une ligne vide et certain ligne contient plus de 2 élément(post,python or r) car certaines lignes contient des caractéres comme ' ,\" qui fait des confusions avec les ' \" de débur et la fin d un str --- solution\n",
    "\n",
    "                rData.append(line)\n",
    "\n",
    "\n",
    "\n",
    "# Lcture des fichiers post_python_x.csv dans une liste pythonData\n",
    "path_python=r\"C:\\Users\\Yassine Z\\Desktop\\Data_Posts_StackOverFlow\\Data_Python\"\n",
    "python_files_name=[\"Posts_Python_Page_{}.csv\".format(i) for i in range(1,3501)]\n",
    "pythonData=[]\n",
    "for file_name in python_files_name :\n",
    "\n",
    "   with open(path_python+\"/\"+file_name, 'r',encoding='utf-8') as readFile:\n",
    "       reader = csv.reader(readFile)\n",
    "       lines = list(reader)\n",
    "       for line in lines[1:] :\n",
    "            if len(line)==2 :                   # car entre une ligne et une autre il y a une ligne vide\n",
    "                pythonData.append(line)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "allData=rData+pythonData              # cette liste contient tous les posts r et python\n",
    "\n",
    "random.shuffle(allData)               # mélanger le contenu dans allData\n",
    "\n",
    "\n",
    "data=pd.DataFrame(allData,columns=header)\n",
    "\n",
    "\n",
    "data=data.drop_duplicates(subset=\"post\")           #supprimer les posts dupliqués\n",
    "print(\"####################################\")\n",
    "\n",
    "#getting training set examples labels\n",
    "y_train=data['python or r'].values\n",
    "x_train=data['post'].values\n",
    "print (\"Unique Classes: \",np.unique(y_train))\n",
    "print (\"Total Number of  Examples: \",x_train.shape)\n",
    "\n",
    "train_data,test_data,train_labels,test_labels=train_test_split(x_train,y_train,shuffle=True,test_size=0.25,random_state=42,stratify=y_train)\n",
    "classes=np.unique(train_labels)\n",
    "\n",
    " ###Training phase....\n",
    "\n",
    "nb=NaiveBayes(classes)\n",
    "print (\"------------------Training In Progress------------------------\")\n",
    "print (\"Training Examples: \",train_data.shape)\n",
    "nb.train(train_data,train_labels)\n",
    "print ('------------------------Training Completed!')\n",
    "\n",
    "# Testing phase\n",
    "\n",
    "pclasses=nb.test(test_data)\n",
    "test_acc=np.sum(pclasses==test_labels)/float(test_labels.shape[0])\n",
    "print (\"Test Set Examples: \",test_labels.shape[0])\n",
    "print (\"Test Set Accuracy: \",test_acc)\n",
    "#pickling nb\n",
    "print (\"------------------Pickling nb------------------------\")\n",
    "f = open(\"my_pickle\", \"wb\")\n",
    "pickle.dump(nb, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "le module pickle permet de sauvegarder dans un fichier, au format binaire,  n'importe quel objet Python.c à d faire la serialisation des objets pour éviter\n",
    "à chaque fois de refaire l'entrainement. l'objet nb qui contient les dictionnaires de python ou de R est serialisé dans le fichier \"nb.pickle\"\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from Naive_Bayes import  NaiveBayes\n",
    "inn=open(\"my_pickle\",\"rb\")\n",
    "nb_pickled=pk.load(inn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# la méthode stringToArray permet d'insérer un string s dans un objet array pour lui passer par la suite à la méthode test de la classe NaiveBayes\n",
    "def stringToArray(s) :\n",
    "    l=[]\n",
    "    l.append(s)\n",
    "    return np.array(l)\n",
    "text=\"\"\"\n",
    "A good data scientist is a passionate coder python python py python thon python -slash-statistician, and there’s no better \n",
    "programming language for a statistician to learn than R. The standard among\n",
    "statistical programming languages, R is sometimes called the “goldenr child” of data science. \n",
    "It’s a popular skill among Big Data analysts, and data scientists skilled in R are sought after by some of the biggest brands, including Google, Facebook, Bank of America and the New York Times.\n",
    "In addition, R’s commercial app python pythonlications increas python python e by the minute python is , and companies appreciate its versatility. If you’re intrigued and want to know why you should learn R, here are a few more reasons why you should add R to your skillset:\n",
    "  \"\"\"\n",
    "text=stringToArray(text)\n",
    "print(type(nb.test(text)))\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
